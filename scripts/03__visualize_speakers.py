from collections import Counter
import json

import networkx as nx
import pandas as pd
import colorcet


# Colors used for different graph modularity classes
COLORS = ["#999999"] + colorcet.glasbey_dark + colorcet.glasbey_dark

OUTPUT_JSON = "../public/speakers.json"

NODE_SCALING = 10
# GraphML file generated by Gephi
INPUT_GRAPHML = "graphml/speakers_layout.graphml"

BOUND = 1000
BOUNDING_BOX = {"x": [-BOUND + 500, BOUND + 500], "y": [-BOUND,BOUND]}

LABEL_THRESHOLD = 8

if __name__ == "__main__":

    G = nx.read_graphml(path=INPUT_GRAPHML)

    _nodes_df = pd.DataFrame.from_dict(
        dict(G.nodes(data=True)), orient="index"
    ).reset_index(drop=True)
    _edges_df = nx.to_pandas_edgelist(G=G)

    nodes_df = _nodes_df[["x", "y", "label", "size", "affiliation"]]
    edges_df = _edges_df[["source", "target"]]

    nodes_df["cluster"] = nodes_df["affiliation"]
    nodes_df["cluster"].fillna("Other", inplace=True)
    nodes_df.drop("affiliation", axis="columns", inplace=True)

    label_to_index = {t[0] : i for i, t in enumerate(Counter(list(edges_df['source']) + list(edges_df['target'])).most_common())}

    nodes_df["key"] = nodes_df['label'].map(label_to_index)
    nodes_df["size"] /= NODE_SCALING
    nodes = nodes_df.to_dict(orient="records")

    edges = [[label_to_index[e["source"]], label_to_index[e["target"]]] for e in edges_df.to_dict(orient="records")]

    clusters = [k for k, _ in Counter(nodes_df["cluster"]).most_common()]

    data = {
        "nodes": nodes,
        "edges": edges,
        "clusters": [
            {"key": cluster, "clusterLabel": cluster, "color": COLORS[i]} for i, cluster in enumerate(clusters)
        ],
        "bbox": BOUNDING_BOX,
        'labelThreshold': LABEL_THRESHOLD
    }

    with open(OUTPUT_JSON, "w") as f:
        json.dump(obj=data, fp=f, separators=(",", ":"))